package algorithms;

import config.AlgorithmConfig;
import model.*;
import utils.DatasetStatistics;
import utils.OptimizedDataStructures;
import utils.UtilityCalculator;

import java.util.*;

public class TKUSP_V7 implements Algorithm {
    private long runtime;
    private double memoryUsage;
    private final Random random;
    private double[] lengthProbabilities; // p(i) for i in [1..max_length_seq]

    /**
     * ‚ö° STRUCTURE POUR G√âN√âRATION RAPIDE DEPUIS PM
     * Pr√©calcule les CDF (Cumulative Distribution Function) pour chaque position
     */
    private double[][] cumulativePM; // CDF pour sampling rapide
    private boolean cdfNeedsUpdate = true;

    public TKUSP_V7() {
        this.random = new Random();
    }

    public TKUSP_V7(long seed) {
        this.random = new Random(seed);
    }

    @Override
    public List<Sequence> run(Dataset dataset, AlgorithmConfig config) {
        long startTime = System.currentTimeMillis();
        Runtime runtime = Runtime.getRuntime();
        runtime.gc();
        long memoryBefore = runtime.totalMemory() - runtime.freeMemory();

        // ‚≠ê INITIALISER L'INDEX COMPACT AU D√âBUT
        UtilityCalculator.initializeCompactIndex(dataset);

        // 1) Statistiques dataset
        DatasetStatistics stats = new DatasetStatistics(dataset);

        // 2) INITIALISER dataStructures AVANT toute utilisation !
        long currentMinUtil = 0L; // ou valeur d'init voulue
        OptimizedDataStructures dataStructures = new OptimizedDataStructures(dataset, currentMinUtil);

        // 4) calculer utilit√©s des singletons
        List<Integer> items = dataStructures.getPromisingItems();
        Map<Integer, Long> singletonUtils = dataStructures.computeSingletonUtilities(items);

        // 5) trier les singletons par utilit√© d√©croissante
        List<Map.Entry<Integer, Long>> singleList = new ArrayList<>(singletonUtils.entrySet());
        singleList.sort((e1, e2) -> Long.compare(e2.getValue(), e1.getValue()));

        // 6) construire topK initial √† partir des k singletons les plus utiles
        List<Sequence> topK = new ArrayList<>();
        int initialCount = Math.min(config.getK(), singleList.size());
        for (int idx = 0; idx < initialCount; idx++) {
            int itemId = singleList.get(idx).getKey();
            long util = singleList.get(idx).getValue();
            Sequence s = new Sequence();
            Itemset is = new Itemset();
            is.addItem(new Item(itemId)); // Item utility = 0 for pattern, sequence utility will be set
            s.addItemset(is);
            s.setUtility((int) util); // store singleton utility as initial cached utility
            topK.add(s);
        }

        // 7) initialiser currentMinUtil avec la k-√®me utilit√© (ou la plus petite si
        // moins de k)
        if (singleList.size() >= config.getK()) {
            currentMinUtil = singleList.get(config.getK() - 1).getValue();
        } else if (!singleList.isEmpty())
            currentMinUtil = singleList.get(singleList.size() - 1).getValue();

        // 8) Pruner les items : reconstruire promising items / index en utilisant
        // currentMinUtil
        dataStructures.updatePromisingItems(currentMinUtil);
        // r√©duire le cache par-sequence en supprimant les ids non-prometteurs
        dataStructures.releaseNonPromisingDistinctIds();

        // Vider les caches d'utilit√© (les patterns pr√©c√©demment calcul√©s peuvent ne
        // plus √™tre pertinents)
        UtilityCalculator.clearCache();

        // 9) reconstruire la liste active d'items et initialiser PM sur les items
        // prometteurs
        items = dataStructures.getPromisingItems();
        double[][] PM = initializeProbabilityMatrix(
                items.size(),
                config.getMaxSequenceLength());

        // Initialize sequence length probability uniformly
        lengthProbabilities = initializeLengthProbabilities(config.getMaxSequenceLength());
        //dataStructures.printStatistics();
        // --------------------------------------------------------------------------
        List<Sequence> elite = null; // elite from previous iteration for smooth factor

        int iteration = 1;

        while (iteration <= config.getMaxIterations() && !isBinaryMatrix(PM)) {
            //System.out.printf("\n Iteration %d ", iteration);

            // Calculate smooth factor from elite (use rho for first iteration)
            double smoothFactor = config.getRho();
            if (iteration > 1 && elite != null && !elite.isEmpty()) {
                smoothFactor = calculateSmoothFactor(elite, config.getRho());
            }

            // G√©n√©rer l'√©chantillon avec smoothing mutation et fusion
            List<Sequence> sample = generateSample(
                    PM,
                    config.getSampleSize(),
                    items,
                    stats,
                    config.getMaxSequenceLength(),
                    smoothFactor,
                    elite,
                    topK);

            // ===== CALCUL OPTIMIS√â DES UTILIT√âS =====
            //long runtime_calculeUtility_start = System.currentTimeMillis();
            for (Sequence seq : sample) {
                long utility = UtilityCalculator.calculateSequenceUtility(seq, dataStructures);
                seq.setUtility((int) utility);
            }
            //long runtime_calculeUtility = System.currentTimeMillis() - runtime_calculeUtility_start;
            //System.out.printf("\n runtime of calculateSequenceUtility = %f s",runtime_calculeUtility/1000.0);
            // =========================================

            // Trier par utilit√© d√©croissante
            sample.sort((s1, s2) -> Integer.compare(s2.getUtility(), s1.getUtility()));

            // S√©lectionner l'√©lite
            int eliteSize = Math.max(1, (int) Math.ceil(config.getRho() * sample.size()));
            elite = sample.subList(0, eliteSize);

            // Mettre √† jour top-k
            topK = updateTopK(topK, sample, config.getK());

            // Mettre √† jour currentMinUtil en utilisant le k-i√®me utilit√© de topK
            long newMinUtilFromTopK = 0;
            if (topK.size() >= config.getK()) {
                newMinUtilFromTopK = topK.get(config.getK() - 1).getUtility();
            } else if (!topK.isEmpty()) {
                newMinUtilFromTopK = topK.get(topK.size() - 1).getUtility();
            }

            if (newMinUtilFromTopK > currentMinUtil) {
                // System.out.printf("TopK increased minUtil: %d -> %d\n", currentMinUtil,
                // newMinUtilFromTopK);
                currentMinUtil = newMinUtilFromTopK;

                // sauvegarder ancien mapping items -> PM (ou juste les items)
                List<Integer> oldItemsForPM = new ArrayList<>(items);
                double[][] oldPMForMerge = copyMatrix(PM);

                // pruning
                dataStructures.updatePromisingItems(currentMinUtil);

                // ‚ö° PRUNING DANS PM
                pruneNonPromisingItemsInPM(PM, items, dataStructures, currentMinUtil);

                dataStructures.releaseNonPromisingDistinctIds();

                // nouvelle liste d'items prometteurs
                items = dataStructures.getPromisingItems();

                // calculer les items supprim√©s
                Set<Integer> removed = new HashSet<>(oldItemsForPM);
                removed.removeAll(items);
                if (!removed.isEmpty()) {
                    // invalider seulement les entr√©es qui contiennent les items supprim√©s
                    UtilityCalculator.invalidateCacheForRemovedItems(removed);
                }

                // reconstruire PM en pr√©servant les lignes des items communs ...
                PM = rebuildProbabilityMatrixPreserving(oldPMForMerge, oldItemsForPM, items,
                        config.getMaxSequenceLength());

                // Filtrer l'√©lite pour retirer les items qui ne sont plus prometteurs
                elite = filterEliteSequences(elite, items);
            }
            //long runtime_updatePM_start = System.currentTimeMillis();
            // Mettre √† jour la matrice de probabilit√©
            PM = updateProbabilityMatrixOptimized(PM, elite, items, config);
            //long runtime_updatePM = System.currentTimeMillis() - runtime_updatePM_start;
            //System.out.printf("\n runtime of updateProbabilityMatrix = %f s",runtime_updatePM/1000.0);

            // Update sequence length probabilities based on elite statistics
            lengthProbabilities = updateLengthProbabilities(elite, config.getMaxSequenceLength(), config);

            iteration++;
        }

        // ‚ö° AFFICHER LES STATISTIQUES √Ä LA FIN
        //UtilityCalculator.printCacheStatistics();

        //printProbabilityMatrix(PM,items);

        dataStructures.releasePerSequenceDistinctIds();
        long endTime = System.currentTimeMillis();
        this.runtime = endTime - startTime;

        long memoryAfter = runtime.totalMemory() - runtime.freeMemory();
        this.memoryUsage = (memoryAfter - memoryBefore) / (1024.0 * 1024.0);

        System.out.println("=== Algorithm Completed ===");
        //System.out.printf("Total iterations: %d\n", iteration - 1);
        System.out.printf("Runtime: %.2f s\n", this.runtime / 1000.0);
        System.out.printf("Memory: %.2f MB\n", this.memoryUsage);
        System.out.println();
        //UtilityCalculator.printCacheStatistics();
        return topK;
    }

    /**
     * Copie profonde d'une matrice double.
     */
    private double[][] copyMatrix(double[][] src) {
        if (src == null)
            return null;
        int rows = src.length;
        int cols = src[0].length;
        double[][] dst = new double[rows][cols];
        for (int i = 0; i < rows; i++) {
            System.arraycopy(src[i], 0, dst[i], 0, src[i].length);
        }
        return dst;
    }

    /**
     * Reconstruit une PM de taille newItems.size() x maxCols :
     * - copie les lignes de oldPM correspondant aux itemIds communs (par id),
     * - initialise √† defaultProb les lignes des nouveaux items,
     * - copie au plus min(oldCols, maxCols) colonnes (oldCols peut √™tre √©gal √†
     * maxCols).
     */
    private double[][] rebuildProbabilityMatrixPreserving(
            double[][] oldPM,
            List<Integer> oldItems,
            List<Integer> newItems,
            int maxCols) {

        // Hypoth√®ses : oldPM != null, oldItems != null, newItems subset of oldItems,
        // oldPM[0].length == maxCols, et newItems.size() <= oldItems.size()

        int newRows = newItems.size();
        double[][] newPM = new double[newRows][maxCols];

        // initialiser √† defaultProb
        for (int i = 0; i < newRows; i++) {
            Arrays.fill(newPM[i], 0.5);
        }

        // construire map itemId -> oldIndex (n√©cessaire pour aligner par id)
        Map<Integer, Integer> oldIndex = new HashMap<>(oldItems.size());
        for (int i = 0; i < oldItems.size(); i++)
            oldIndex.put(oldItems.get(i), i);

        // copier directement maxCols colonnes (on assume oldPM[oi].length == maxCols)
        for (int r = 0; r < newRows; r++) {
            Integer oi = oldIndex.get(newItems.get(r));
            if (oi != null) {
                System.arraycopy(oldPM[oi], 0, newPM[r], 0, maxCols);
            }
        }

        return newPM;
    }

    private double[][] initializeProbabilityMatrix(int numItems, int maxLength) {
        double[][] PM = new double[numItems][maxLength];
        for (int i = 0; i < numItems; i++) {
            for (int j = 0; j < maxLength; j++) {
                PM[i][j] = 0.5;
            }
        }
        return PM;
    }

    /**
     * Initialize sequence length probabilities uniformly.
     * p(i) = 1 / max_length_seq for all i in [1..max_length_seq]
     */
    private double[] initializeLengthProbabilities(int maxLength) {
        double[] probs = new double[maxLength];
        double uniformProb = 1.0 / maxLength;
        Arrays.fill(probs, uniformProb);
        return probs;
    }

    /**
     * Update sequence length probabilities based on elite statistics.
     * p_new(i) = (occ(i) + 1) / (elite_size + max_length_seq)
     * where occ(i) = number of sequences of length i in elite
     */
    private double[] updateLengthProbabilities(List<Sequence> elite, int maxLength, AlgorithmConfig config) {
        int eliteSize = elite.size();
        double[] newProbs = new double[maxLength];

        // Count occurrences of each length in elite
        int[] occurrences = new int[maxLength];
        for (Sequence seq : elite) {
            int len = seq.length();
            if (len >= 1 && len <= maxLength) {
                occurrences[len - 1]++; // length i maps to index i-1
            }
        }

        // Calculate frequency in elite (with Laplace smoothing)
        double denominator = eliteSize + maxLength;

        for (int i = 0; i < maxLength; i++) {
            double frequency = (occurrences[i] + 1) / denominator;

            // 1. Apply Learning Rate (Alpha)
            // P_new = (1 - alpha) * P_old + alpha * P_elite
            newProbs[i] = (1.0 - config.getLearningRate()) * lengthProbabilities[i]
                    + config.getLearningRate() * frequency;

            // 2. Apply Minimum Probability Bound
            // Ensure at least minProbability (or a small fraction like 0.01) for each
            // length
            //double minLenProb = Math.max(config.getMinProbability(), 0.01);
            newProbs[i] = Math.max(0.05, newProbs[i]);
        }

        // 3. Renormalize to ensure sum is 1.0
        double sum = 0.0;
        for (double p : newProbs)
            sum += p;

        for (int i = 0; i < maxLength; i++) {
            newProbs[i] /= sum;
        }

        return newProbs;
    }

    /**
     * Sample a sequence length from the current probability distribution.
     * Uses cumulative distribution function for sampling.
     */
    private int sampleSequenceLength() {
        double r = random.nextDouble();
        double cumulative = 0.0;

        for (int i = 0; i < lengthProbabilities.length; i++) {
            cumulative += lengthProbabilities[i];
            if (r <= cumulative) {
                return i + 1; // length = index + 1
            }
        }

        // Fallback to last length (should rarely happen due to rounding)
        return lengthProbabilities.length;
    }

    private boolean isBinaryMatrix(double[][] PM) {
        for (double[] row : PM) {
            for (double val : row) {
                if (val != 0.0 && val != 1.0) {
                    return false;
                }
            }
        }
        return true;
    }

    /**
     * Calculate smooth factor Œ± based on elite diversity.
     * Œ± = [(u_best ‚àí u_quantile) / u_best] √ó œÅ
     */
    private double calculateSmoothFactor(List<Sequence> elite, double rho) {
        if (elite == null || elite.isEmpty())
            return rho;

        int eliteSize = elite.size();
        int quantileIdx = (int) Math.floor(eliteSize * rho);
        if (quantileIdx >= eliteSize)
            quantileIdx = eliteSize - 1;

        int uBest = elite.get(0).getUtility(); // elite is sorted descending
        int uQuantile = elite.get(quantileIdx).getUtility();

        if (uBest == 0)
            return rho; // avoid division by zero

        double alpha = ((double) (uBest - uQuantile) / uBest) * rho;
        return Math.max(0.0, Math.min(alpha, 1.0)); // clamp to [0,1]
    }

    /**
     * Generate a completely random sequence for exploration.
     * Uses uniform random length and random items from promising items.
     */
    private Sequence generateRandomSequence(int maxLength, List<Integer> items,
                                            DatasetStatistics stats) {
        // Random sequence length (uniform for true exploration)
        int seqLength = 1 + random.nextInt(maxLength);

        Sequence sequence = new Sequence();

        for (int pos = 0; pos < seqLength; pos++) {
            // Random itemset size from dataset distribution
            int itemsetSize = stats.sampleItemsetSize(random);
            if (itemsetSize <= 0)
                itemsetSize = 1;

            // Random items from promising items
            List<Item> chosenItems = new ArrayList<>();
            int maxAttempts = Math.min(itemsetSize, items.size());

            Set<Integer> selectedIds = new HashSet<>();
            for (int i = 0; i < maxAttempts; i++) {
                int randomIdx = random.nextInt(items.size());
                int itemId = items.get(randomIdx);
                if (selectedIds.add(itemId)) {
                    chosenItems.add(new Item(itemId));
                }
            }

            if (!chosenItems.isEmpty()) {
                sequence.addItemset(new Itemset(chosenItems));
            }
        }

        return sequence;
    }

    private List<Sequence> generateSample(double[][] PM, int N, List<Integer> items,
                                          DatasetStatistics stats, int maxLength, double smoothFactor,
                                          List<Sequence> elite, List<Sequence> topK) {
        List<Sequence> sample = new ArrayList<>();

        // Param√®tres de g√©n√©ration
        double fusionRatio = 0.10; // 10% pour fusion-based exploration
        int numFusion = (int) Math.floor(N * fusionRatio);
        int numRandom = (int) Math.floor(N * smoothFactor);
        int numPMBased = N - numFusion - numRandom;

        // 1. Generate fusion-based sequences (exploration via crossover)
        if (numFusion > 0 && elite != null && !elite.isEmpty()) {
            List<Sequence> fusionSeqs = generateFusionSequences(elite, topK, numFusion, maxLength);
            sample.addAll(fusionSeqs);
        }

        // 2. Generate random sequences (exploration)
        for (int i = 0; i < numRandom; i++) {
            Sequence seq = generateRandomSequence(maxLength, items, stats);
            if (!seq.isEmpty()) {
                sample.add(seq);
            }
        }

        // 3. Generate PM-based sequences (exploitation)
        for (int i = 0; i < numPMBased; i++) {
            // Sample sequence length from learned distribution
            int seqLength = sampleSequenceLength();
            // System.out.println(seqLength);
            Sequence sequence = new Sequence();

            for (int pos = 0; pos < seqLength; pos++) {
                // G√©n√©rer un itemset pour cette position
                Itemset itemset = generateItemsetOptimized(PM, items, pos, stats);

                if (!itemset.isEmpty()) {
                    sequence.addItemset(itemset);
                }
            }

            if (!sequence.isEmpty()) {
                sample.add(sequence);
            }
        }

        return sample;
    }

    /**
     * G√©n√®re des s√©quences par fusion (crossover) de deux s√©quences existantes.
     * S√©lectionne des s√©quences de taille < maxLength/2 et les fusionne dans les
     * deux ordres.
     */
    private List<Sequence> generateFusionSequences(List<Sequence> elite, List<Sequence> topK,
                                                   int numFusion, int maxLength) {
        List<Sequence> fusionSeqs = new ArrayList<>();
        Set<String> signatures = new HashSet<>(); // Pour √©viter les doublons

        // Collecter les candidats: s√©quences de longueur <= maxLength/2
        List<Sequence> candidates = new ArrayList<>();
        int maxLenForFusion = maxLength / 2;

        // Ajouter d'abord les s√©quences de l'√©lite
        if (elite != null) {
            for (Sequence seq : elite) {
                if (seq.length() > 0 && seq.length() <= maxLenForFusion) {
                    candidates.add(seq);
                }
            }
        }

        // Si pas assez de candidats, retourner une liste vide
        if (candidates.size() < 2) {
            return fusionSeqs;
        }

        // G√©n√©rer les fusions
        int attempts = 0;
        int maxAttempts = numFusion * 3; // √âviter boucle infinie

        while (fusionSeqs.size() < numFusion && attempts < maxAttempts) {
            attempts++;

            // S√©lectionner deux s√©quences al√©atoires
            int idx1 = random.nextInt(candidates.size());
            int idx2 = random.nextInt(candidates.size());

            if (idx1 == idx2 && candidates.size() > 1) {
                continue; // √âviter de fusionner une s√©quence avec elle-m√™me
            }

            Sequence s1 = candidates.get(idx1);
            Sequence s2 = candidates.get(idx2);

            // V√©rifier que la fusion ne d√©passe pas maxLength
            if (s1.length() + s2.length() > maxLength) {
                continue;
            }

            // Fusion dans les deux ordres: s1+s2 et s2+s1
            Sequence fusion1 = fuseSequences(s1, s2);
            Sequence fusion2 = fuseSequences(s2, s1);

            // Ajouter fusion1 si non dupliqu√©e
            if (!fusion1.isEmpty()) {
                String sig1 = fusion1.getSignature();
                if (signatures.add(sig1)) {
                    fusionSeqs.add(fusion1);
                }
            }

            // Ajouter fusion2 si non dupliqu√©e et diff√©rente de fusion1
            if (fusionSeqs.size() < numFusion && !fusion2.isEmpty()) {
                String sig2 = fusion2.getSignature();
                if (signatures.add(sig2)) {
                    fusionSeqs.add(fusion2);
                }
            }
        }

        return fusionSeqs;
    }

    /**
     * Fusionne deux s√©quences en concat√©nant leurs itemsets.
     */
    private Sequence fuseSequences(Sequence s1, Sequence s2) {
        Sequence fused = new Sequence();

        // Copier tous les itemsets de s1
        for (Itemset itemset : s1.getItemsets()) {
            Itemset copy = new Itemset();
            for (Item item : itemset.getItems()) {
                copy.addItem(new Item(item.getId()));
            }
            fused.addItemset(copy);
        }

        // Copier tous les itemsets de s2
        for (Itemset itemset : s2.getItemsets()) {
            Itemset copy = new Itemset();
            for (Item item : itemset.getItems()) {
                copy.addItem(new Item(item.getId()));
            }
            fused.addItemset(copy);
        }

        return fused;
    }

    private Itemset generateItemset(double[][] PM, List<Integer> items, int position, DatasetStatistics stats) {
        List<Item> chosenItems = new ArrayList<>();

        // Construire d'abord la liste des candidats (comme avant)
        for (int itemIdx = 0; itemIdx < items.size(); itemIdx++) {
            if (random.nextDouble() < PM[itemIdx][position]) {
                int itemId = items.get(itemIdx);
                chosenItems.add(new Item(itemId));
            }
        }

        int maxElemSize = stats.sampleItemsetSize(random);
        if (maxElemSize <= 0)
            maxElemSize = 1;

        // Si plus de candidats que k, faire un Fisher-Yates PARTIEL :
        // pour i in [0..k-1] swap chosenItems[i] avec chosenItems[i + rnd(0..n-i-1)]
        if (chosenItems.size() > maxElemSize) {
            int n = chosenItems.size();
            for (int i = 0; i < maxElemSize; i++) {
                int j = i + random.nextInt(n - i); // j in [i, n-1]
                // swap elements i et j
                Item tmp = chosenItems.get(i);
                chosenItems.set(i, chosenItems.get(j));
                chosenItems.set(j, tmp);
            }
            // Gagner du temps : on ne fait que k swaps (au lieu de n swaps d'un shuffle
            // complet)
            chosenItems = chosenItems.subList(0, maxElemSize);
        }

        // Fallback si aucun item choisi
        if (chosenItems.isEmpty()) {
            chosenItems.add(fallbackItem(PM, items, position));
        }
        // System.out.printf(" size itemset = %d ",chosenItems.size());

        return new Itemset(chosenItems);
    }

    private Item fallbackItem(double[][] PM, List<Integer> items, int position) {
        double sum = 0.0;
        for (int i = 0; i < items.size(); i++) {
            sum += PM[i][position];
        }

        if (sum == 0.0) {
            int itemId = items.get(random.nextInt(items.size()));
            return new Item(itemId);
        }

        double r = random.nextDouble() * sum;
        double cumulative = 0.0;

        for (int i = 0; i < items.size(); i++) {
            cumulative += PM[i][position];
            if (cumulative >= r) {
                return new Item(items.get(i));
            }
        }

        return new Item(items.get(0));
    }

    private List<Sequence> updateTopK(List<Sequence> currentTopK, List<Sequence> sample, int k) {

        // Min-heap : la plus petite utilit√© est en t√™te
        PriorityQueue<Sequence> pq = new PriorityQueue<>(
                Comparator.comparingInt(Sequence::getUtility));

        // D√©duplication par signature (√©vite d'ajouter la m√™me s√©quence plusieurs fois)
        Set<String> seen = new HashSet<>();

        // Ajoute les √©l√©ments actuels
        for (Sequence seq : currentTopK) {
            String sig = seq.getSignature();
            if (seen.add(sig)) {
                pq.offer(seq);
                if (pq.size() > k) { // si d√©pass√©, on retire le plus petit
                    pq.poll();
                }
            }
        }

        // Ajoute les nouveaux candidats (sample)
        for (Sequence seq : sample) {
            String sig = seq.getSignature();
            if (seen.add(sig)) {
                pq.offer(seq);
                if (pq.size() > k) {
                    pq.poll();
                }
            }
        }

        // R√©cup√©rer les √©l√©ments du heap dans l'ordre d√©croissant
        List<Sequence> result = new ArrayList<>(pq);
        result.sort((a, b) -> Integer.compare(b.getUtility(), a.getUtility()));
        return result;
    }

    /**
     * Filtre les s√©quences de l'√©lite en ne gardant que les items prometteurs.
     * Retourne une nouvelle liste de s√©quences filtr√©es.
     */
    private List<Sequence> filterEliteSequences(List<Sequence> elite, List<Integer> promisingItems) {
        Set<Integer> promisingSet = new HashSet<>(promisingItems);
        List<Sequence> filteredElite = new ArrayList<>();

        for (Sequence seq : elite) {
            Sequence filteredSeq = new Sequence();
            filteredSeq.setUtility(seq.getUtility());

            for (Itemset itemset : seq.getItemsets()) {
                Itemset filteredItemset = new Itemset();

                for (Item item : itemset.getItems()) {
                    if (promisingSet.contains(item.getId())) {
                        filteredItemset.addItem(item);
                    }
                }

                // N'ajouter l'itemset que s'il n'est pas vide
                if (!filteredItemset.isEmpty()) {
                    filteredSeq.addItemset(filteredItemset);
                }
            }

            // N'ajouter la s√©quence que si elle n'est pas vide
            if (!filteredSeq.isEmpty()) {
                filteredElite.add(filteredSeq);
            }
        }

        return filteredElite;
    }

    private double[][] updateProbabilityMatrix(double[][] PM, List<Sequence> elite, List<Integer> items,
                                               AlgorithmConfig config) {
        double[][] newPM = new double[PM.length][PM[0].length];

        for (int itemIdx = 0; itemIdx < items.size(); itemIdx++) {
            int itemId = items.get(itemIdx);

            for (int pos = 0; pos < PM[0].length; pos++) {
                int count = 0;
                int total = 0;

                for (Sequence seq : elite) {
                    if (pos < seq.length()) {
                        total++;
                        Itemset itemset = seq.getItemsets().get(pos);

                        // V√©rifier si l'item est dans cet itemset
                        boolean containsItem = itemset.getItems().stream()
                                .anyMatch(item -> item.getId() == itemId);

                        if (containsItem) {
                            count++;
                        }
                    }
                }

                // Calculer la nouvelle probabilit√©
                double probability = (total > 0) ? (double) count / total : PM[itemIdx][pos];

                // 1. Application du Learning Rate (Alpha)
                double newProb = (1.0 - config.getLearningRate()) * PM[itemIdx][pos]
                        + config.getLearningRate() * probability;

                newPM[itemIdx][pos] = newProb;
            }
        }

        // ‚ö° MARQUER CDF POUR MISE √Ä JOUR
        cdfNeedsUpdate = true;

        return newPM;
    }

    /**
     * Remplace la version na√Øve : parcourt l'√©lite puis accroit des compteurs.
     * Avantage : on √©vite de parcourir items √ó elite.
     */
    private double[][] updateProbabilityMatrixOptimized(double[][] PM,
                                                        List<Sequence> elite,
                                                        List<Integer> items,
                                                        AlgorithmConfig config) {

        if (elite == null || elite.isEmpty()) {
            return PM; // rien √† apprendre
        }

        final int numItems = items.size();
        final int maxLen = PM[0].length;
        final double alpha = config.getLearningRate();

        // map itemId -> index dans "items"
        Map<Integer, Integer> itemIndex = new HashMap<>(numItems);
        for (int i = 0; i < numItems; i++) itemIndex.put(items.get(i), i);

        // counts[itemIdx][pos] et totals[pos]
        int[][] counts = new int[numItems][maxLen];
        int[] totals = new int[maxLen];

        // 1) Agr√©gation : parcourir l'√©lite (beaucoup plus petit que items)
        for (Sequence seq : elite) {
            int seqLen = Math.min(seq.length(), maxLen);
            for (int pos = 0; pos < seqLen; pos++) {
                totals[pos]++; // nombre de s√©quences de l'√©lite qui ont cette position
                Itemset is = seq.getItemsets().get(pos);
                for (model.Item it : is.getItems()) {
                    Integer idx = itemIndex.get(it.getId());
                    if (idx != null) counts[idx][pos]++;
                }
            }
        }

        // 2) Construire la nouvelle PM ‚Äî on peut choisir de ne pas toucher les z√©ros
        double[][] newPM = new double[numItems][maxLen];
        for (int i = 0; i < numItems; i++) {
            for (int p = 0; p < maxLen; p++) {
                if (totals[p] > 0 && counts[i][p] > 0) {
                    double prob = (double) counts[i][p] / totals[p];
                    newPM[i][p] = (1.0 - alpha) * PM[i][p] + alpha * prob;
                } else {
                    // Option 1 (cheap) : ne pas toucher la cellule si l'item n'est jamais
                    // apparu √† cette position dans l'√©lite -> √©vite d'√©crire toutes les
                    // cellules z√©ro (gain important quand elite << items)
                    //newPM[i][p] = PM[i][p];

                    // Option 2 (si tu veux d√©cro√Ætre lentement les items inactifs) :
                    newPM[i][p] = (1.0 - alpha) * PM[i][p];
                    //newPM[i][p] = alpha * PM[i][p];
                }
            }
        }

        cdfNeedsUpdate = true;
        return newPM;
    }

    /**
     * ‚ö° PR√âCALCUL DES CDF (appel√© apr√®s chaque mise √† jour de PM)
     */
    private void updateCumulativePM(double[][] PM) {
        int numItems = PM.length;
        int maxLength = PM[0].length;

        cumulativePM = new double[maxLength][numItems];

        for (int pos = 0; pos < maxLength; pos++) {
            double sum = 0.0;
            for (int i = 0; i < numItems; i++) {
                sum += PM[i][pos];
                cumulativePM[pos][i] = sum;
            }
        }

        cdfNeedsUpdate = false;
    }

    /**
     * ‚ö° G√âN√âRATION D'ITEMSET OPTIMIS√âE
     *
     * ALGORITHME :
     * 1. Tirer un nombre k (taille d'itemset) depuis la distribution empirique
     * 2. Pour chaque item, faire un sampling biais√© avec recherche binaire dans CDF
     *
     * COMPLEXIT√â : O(k √ó log(|items|)) au lieu de O(|items|)
     */
    private Itemset generateItemsetOptimized(double[][] PM, List<Integer> items,
                                             int position, DatasetStatistics stats) {

        // ‚ö° PR√âCALCUL CDF SI N√âCESSAIRE
        if (cdfNeedsUpdate) {
            updateCumulativePM(PM);
        }

        // √âchantillonner la taille d'itemset
        int targetSize = stats.sampleItemsetSize(random);
        if (targetSize <= 0) targetSize = 1;

        List<Item> chosenItems = new ArrayList<>();
        Set<Integer> alreadyChosen = new HashSet<>();

        // ‚ö° G√âN√âRATION AVEC RECHERCHE BINAIRE (O(log n) par item)
        double totalProb = cumulativePM[position][items.size() - 1];

        int attempts = 0;
        while (chosenItems.size() < targetSize && attempts < targetSize * 3) {
            attempts++;

            // Tirer un nombre al√©atoire [0, totalProb]
            double r = random.nextDouble() * totalProb;

            // ‚ö° RECHERCHE BINAIRE dans le CDF
            int itemIdx = binarySearchCDF(cumulativePM[position], r, items.size());

            if (itemIdx >= 0 && itemIdx < items.size()) {
                int itemId = items.get(itemIdx);

                if (alreadyChosen.add(itemId)) { // √âviter doublons
                    chosenItems.add(new Item(itemId));
                }
            }
        }

        // Fallback si aucun item s√©lectionn√©
        if (chosenItems.isEmpty()) {
            chosenItems.add(fallbackItem(PM, items, position));
        }

        return new Itemset(chosenItems);
    }

    /**
     * ‚ö° RECHERCHE BINAIRE dans le CDF
     */
    private int binarySearchCDF(double[] cdf, double target, int size) {
        int left = 0, right = size - 1;

        while (left < right) {
            int mid = (left + right) / 2;
            if (cdf[mid] < target) {
                left = mid + 1;
            } else {
                right = mid;
            }
        }

        return left;
    }

    /**
     * ‚ö° √âLAGAGE DES ITEMS NON PROMETTEURS DANS PM
     * Inspir√© de IIP (Irrelevant Items Pruning) de HUSP-SP
     *
     * ALGORITHME :
     * 1. Pour chaque item i, calculer RSU (Reduced Sequence Utility)
     * 2. Si RSU(i) < currentMinUtil, mettre PM[i][*] = 0 (ignorer cet item)
     */
    private void pruneNonPromisingItemsInPM(double[][] PM, List<Integer> items,
                                            OptimizedDataStructures dataStructures,
                                            long currentMinUtil) {

        //System.out.println("üîç Pruning non-promising items in PM (IIP strategy)...");
        int prunedCount = 0;

        for (int itemIdx = 0; itemIdx < items.size(); itemIdx++) {
            int itemId = items.get(itemIdx);
            long swu = dataStructures.getSWU(itemId);

            // ‚ö° SI SWU < minUtil, cet item ne peut PAS contribuer √† un top-k pattern
            if (swu < currentMinUtil) {
                // Mettre toutes les probabilit√©s √† 0
                Arrays.fill(PM[itemIdx], 0.0);
                prunedCount++;
            }
        }

        //System.out.printf("‚úÇÔ∏è Pruned %d / %d items (%.1f%%)\n", prunedCount, items.size(), 100.0 * prunedCount / items.size());

        // ‚ö° MARQUER CDF POUR MISE √Ä JOUR
        cdfNeedsUpdate = true;
    }

    /**
     * Affiche la matrice de probabilit√© PM de mani√®re format√©e.
     */
    private void printProbabilityMatrix(double[][] PM, List<Integer> items) {
        if (PM == null || PM.length == 0) {
            System.out.println("Matrice PM vide.");
            return;
        }

        int numItems = PM.length;
        int maxLength = PM[0].length;

        System.out.printf("Dimensions: %d items x %d positions\n", numItems, maxLength);

        // Afficher seulement les premi√®res lignes et colonnes si la matrice est grande
        int maxRowsToShow = Math.min(10, numItems);
        int maxColsToShow = Math.min(10, maxLength);

        // En-t√™te avec les positions
        System.out.print("Item\\Pos\t");
        for (int j = 0; j < maxColsToShow; j++) {
            System.out.printf("P%d\t", j);
        }
        if (maxColsToShow < maxLength) {
            System.out.print("...");
        }
        System.out.println();

        // Ligne de s√©paration
        System.out.println("--------" + "--------".repeat(maxColsToShow));

        // Afficher les lignes
        for (int i = 0; i < maxRowsToShow; i++) {
            System.out.printf("Item %d\t", items.get(i));
            for (int j = 0; j < maxColsToShow; j++) {
                System.out.printf("%.2f\t", PM[i][j]);
            }
            if (maxColsToShow < maxLength) {
                System.out.print("...");
            }
            System.out.println();
        }

        if (maxRowsToShow < numItems) {
            System.out.println("...");
        }
        System.out.println();
    }

    @Override
    public String getName() {
        return "TKU-SP (Top-K Utility Sequential Patterns)";
    }

    @Override
    public long getRuntime() {
        return runtime;
    }

    @Override
    public double getMemoryUsage() {
        return memoryUsage;
    }
}